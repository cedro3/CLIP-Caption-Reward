{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP_reward",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/CLIP-Caption-Reward/blob/main/CLIP_reward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JMqvnesGnXG"
      },
      "source": [
        "#@title **setup**\n",
        "\n",
        "# Git clone\n",
        "! git clone https://github.com/cedro3/CLIP-Caption-Reward.git\n",
        "%cd /content/CLIP-Caption-Reward\n",
        "\n",
        "# Install library\n",
        "!pip install -r requirements.txt\n",
        "!pip uninstall -y  torchtext # to bypass pt-lightning issue (https://github.com/PyTorchLightning/pytorch-lightning/issues/6415)\n",
        "!pip install -e .\n",
        "\n",
        "# Import library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import json\n",
        "import captioning.utils.opts as opts\n",
        "import captioning.models as models\n",
        "import captioning.utils.misc as utils\n",
        "import pytorch_lightning as pl\n",
        "from function import display_pic\n",
        "\n",
        "# Checkpoint class\n",
        "class ModelCheckpoint(pl.callbacks.ModelCheckpoint):\n",
        "    def on_keyboard_interrupt(self, trainer, pl_module):\n",
        "        # Save model when keyboard interrupt\n",
        "        filepath = os.path.join(self.dirpath, self.prefix + 'interrupt.ckpt')\n",
        "        self._save_model(filepath)\n",
        "\n",
        "# Device and model configurations\n",
        "device = 'cuda' #@param [\"cuda\", \"cpu\"] {allow-input: true}\n",
        "reward = 'clips_grammar' #@param [\"mle\", \"cider\", \"clips\", \"cider_clips\", \"clips_grammar\"] {allow-input: true}\n",
        "\n",
        "if reward == 'mle':\n",
        "    cfg = f'./configs/phase1/clipRN50_{reward}.yml'\n",
        "else:\n",
        "    cfg = f'./configs/phase2/clipRN50_{reward}.yml'\n",
        "print(\"Loading cfg from\", cfg)\n",
        "\n",
        "opt = opts.parse_opt(parse=False, cfg=cfg)\n",
        "\n",
        "\n",
        "# Doenload pretraied checkpoint\n",
        "import gdown\n",
        "if reward == \"mle\":\n",
        "  url = \"https://drive.google.com/drive/folders/1hfHWDn5iXsdjB63E5zdZBAoRLWHQC3LD\"\n",
        "elif reward == \"cider\":\n",
        "  url = \"https://drive.google.com/drive/folders/1MnSmCd8HFnBvQq_4K-q4vsVkzEw0OIOs\"\n",
        "elif reward == \"clips\":\n",
        "  url = \"https://drive.google.com/drive/folders/1toceycN-qilHsbYjKalBLtHJck1acQVe\"\n",
        "elif reward == \"cider_clips\":\n",
        "  url = \"https://drive.google.com/drive/folders/1toceycN-qilHsbYjKalBLtHJck1acQVe\"\n",
        "elif reward == \"clips_grammar\":\n",
        "  url = \"https://drive.google.com/drive/folders/1nSX9aS7pPK4-OTHYtsUD_uEkwIQVIV7W\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False, output=\"save/\")\n",
        "\n",
        "\n",
        "# --- Load vocabulary ---\n",
        "url = \"https://drive.google.com/uc?id=1HNRE1MYO9wxmtMHLC8zURraoNFu157Dp\"\n",
        "gdown.download(url, quiet=True, use_cookies=False, output=\"data/\")\n",
        "\n",
        "dict_json = json.load(open('./data/cocotalk.json'))\n",
        "print(dict_json.keys())\n",
        "\n",
        "ix_to_word = dict_json['ix_to_word']\n",
        "vocab_size = len(ix_to_word)\n",
        "print('vocab size:', vocab_size)\n",
        "\n",
        "seq_length = 1\n",
        "opt.vocab_size = vocab_size\n",
        "opt.seq_length = seq_length\n",
        "\n",
        "\n",
        "# --- Load Model checkpoint ---\n",
        "opt.batch_size = 1\n",
        "opt.vocab = ix_to_word\n",
        "# opt.use_grammar = False\n",
        "\n",
        "model = models.setup(opt)\n",
        "del opt.vocab\n",
        "\n",
        "ckpt_path = opt.checkpoint_path + '-last.ckpt'\n",
        "\n",
        "print(\"Loading checkpoint from\", ckpt_path)\n",
        "raw_state_dict = torch.load(\n",
        "    ckpt_path,\n",
        "    map_location=device)\n",
        "\n",
        "strict = True\n",
        "\n",
        "state_dict = raw_state_dict['state_dict']\n",
        "\n",
        "if '_vocab' in state_dict:\n",
        "    model.vocab = utils.deserialize(state_dict['_vocab'])\n",
        "    del state_dict['_vocab']\n",
        "elif strict:\n",
        "    raise KeyError\n",
        "if '_opt' in state_dict:\n",
        "    saved_model_opt = utils.deserialize(state_dict['_opt'])\n",
        "    del state_dict['_opt']\n",
        "    # Make sure the saved opt is compatible with the curren topt\n",
        "    need_be_same = [\"caption_model\",\n",
        "                    \"rnn_type\", \"rnn_size\", \"num_layers\"]\n",
        "    for checkme in need_be_same:\n",
        "        if getattr(saved_model_opt, checkme) in ['updown', 'topdown'] and \\\n",
        "                getattr(opt, checkme) in ['updown', 'topdown']:\n",
        "            continue\n",
        "        assert getattr(saved_model_opt, checkme) == getattr(\n",
        "            opt, checkme), \"Command line argument and saved model disagree on '%s' \" % checkme\n",
        "elif strict:\n",
        "    raise KeyError\n",
        "res = model.load_state_dict(state_dict, strict)\n",
        "print(res)\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval();\n",
        "\n",
        "\n",
        "# --- Load CLIP image encoder ---\n",
        "import clip\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "from timm.models.vision_transformer import resize_pos_embed\n",
        "\n",
        "clip_model, clip_transform = clip.load(\"RN50\", jit=False, device=device)\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize((448, 448), interpolation=Image.BICUBIC),\n",
        "    CenterCrop((448, 448)),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.Tensor([0.48145466, 0.4578275, 0.40821073]).to(device).reshape(3, 1, 1)\n",
        "image_std = torch.Tensor([0.26862954, 0.26130258, 0.27577711]).to(device).reshape(3, 1, 1)\n",
        "\n",
        "num_patches = 196 #600 * 1000 // 32 // 32\n",
        "pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, clip_model.visual.attnpool.positional_embedding.shape[-1],  device=device),)\n",
        "pos_embed.weight = resize_pos_embed(clip_model.visual.attnpool.positional_embedding.unsqueeze(0), pos_embed)\n",
        "clip_model.visual.attnpool.positional_embedding = pos_embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **display sample picture**\n",
        "display_pic('images')"
      ],
      "metadata": {
        "id": "xhB3M-mYE4Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Generate caption**\n",
        "\n",
        "# --- Extract visual feature ---\n",
        "picture = '04.jpg'#@param {type:\"string\"}\n",
        "img_path = 'images/'+picture\n",
        "\n",
        "# display image\n",
        "from IPython.display import Image as show_imge\n",
        "from IPython.display import display\n",
        "display(show_imge(img_path))\n",
        "\n",
        "with torch.no_grad():\n",
        "    image = preprocess(Image.open( img_path ).convert(\"RGB\"))\n",
        "    image = torch.tensor(np.stack([image])).to(device)\n",
        "    image -= image_mean\n",
        "    image /= image_std\n",
        "    \n",
        "    tmp_att, tmp_fc = clip_model.encode_image(image)\n",
        "    tmp_att = tmp_att[0].permute(1, 2, 0)\n",
        "    tmp_fc = tmp_fc[0]\n",
        "    \n",
        "    att_feat = tmp_att\n",
        "    fc_feat = tmp_fc\n",
        "\n",
        "\n",
        "# --- Generate caption ---\n",
        "# Inference configurations\n",
        "eval_kwargs = {}\n",
        "eval_kwargs.update(vars(opt))\n",
        "\n",
        "verbose = eval_kwargs.get('verbose', True)\n",
        "verbose_beam = eval_kwargs.get('verbose_beam', 0)\n",
        "verbose_loss = eval_kwargs.get('verbose_loss', 1)\n",
        "\n",
        "# dataset = eval_kwargs.get('dataset', 'coco')\n",
        "beam_size = eval_kwargs.get('beam_size', 1)\n",
        "sample_n = eval_kwargs.get('sample_n', 1)\n",
        "remove_bad_endings = eval_kwargs.get('remove_bad_endings', 0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    fc_feats = torch.zeros((1,0)).to(device)\n",
        "    att_feats = att_feat.view(1, 196, 2048).float().to(device)\n",
        "    att_masks = None\n",
        "\n",
        "    # forward the model to also get generated samples for each image\n",
        "    # Only leave one feature for each image, in case duplicate sample\n",
        "    tmp_eval_kwargs = eval_kwargs.copy()\n",
        "    tmp_eval_kwargs.update({'sample_n': 1})\n",
        "    seq, seq_logprobs = model(\n",
        "        fc_feats, att_feats, att_masks, opt=tmp_eval_kwargs, mode='sample')\n",
        "    seq = seq.data\n",
        "\n",
        "    sents = utils.decode_sequence(model.vocab, seq)\n",
        "\n",
        "print(sents)"
      ],
      "metadata": {
        "id": "yyi2O_q3XLbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}